{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b84407a",
   "metadata": {},
   "source": [
    "Collect data – small labeled set + large unlabeled set.\n",
    "\n",
    "Learn features – train on all data with an unsupervised method (e.g., autoencoder).\n",
    "\n",
    "Add classifier – attach a prediction head and train on labeled data.\n",
    "\n",
    "Use unlabeled data – apply pseudo‑labels or consistency regularization to improve training.\n",
    "\n",
    "Evaluate – test on clean labeled validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb55472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f105a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02180db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "LABELED_SAMPLES   = 1000   # how many true labels you keep from training set\n",
    "CONF_THRESH       = 0.95   # min confidence to accept a pseudo-label\n",
    "# Higher = more reliable, but fewer pseudo-labels, Lower = more pseudo-labels but more noise\n",
    "EPOCHS_BASE       = 5 # Number of epochs for initial supervised training\n",
    "EPOCHS_FINETUNE   = 5 # Number of epochs for fine-tuning with pseudo-labels\n",
    "BATCH_SIZE        = 128 \n",
    "SEED              = 42\n",
    "np.random.seed(SEED); tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bac702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.1612 - loss: 2.2301 - val_accuracy: 0.5536 - val_loss: 1.8815\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.6048 - loss: 1.7044 - val_accuracy: 0.7124 - val_loss: 1.1215\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7466 - loss: 0.9927 - val_accuracy: 0.7872 - val_loss: 0.6789\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8013 - loss: 0.6303 - val_accuracy: 0.8537 - val_loss: 0.4697\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8566 - loss: 0.4338 - val_accuracy: 0.8861 - val_loss: 0.3813\n",
      "Base test accuracy after supervised training on 1000 samples: 0.8861\n",
      "Accepted pseudo-labeled samples: 26425 / 59000 (44.8%)\n",
      "Epoch 1/5\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0180 - val_accuracy: 0.9144 - val_loss: 0.3144\n",
      "Epoch 2/5\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0108 - val_accuracy: 0.9276 - val_loss: 0.2926\n",
      "Epoch 3/5\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9976 - loss: 0.0100 - val_accuracy: 0.9236 - val_loss: 0.3215\n",
      "Epoch 4/5\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0066 - val_accuracy: 0.9344 - val_loss: 0.2799\n",
      "Epoch 5/5\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 0.9362 - val_loss: 0.2802\n",
      "Test accuracy after fine-tuning with pseudo-labels: 0.9362\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() # load data\n",
    "# Normalize and add channel dimension\n",
    "x_train = (x_train.astype(\"float32\") / 255.0)[..., None]\n",
    "x_test  = (x_test.astype(\"float32\") / 255.0)[..., None]\n",
    "\n",
    "# Split small labeled subset ; rest is unlabeled\n",
    "idx = np.random.permutation(len(x_train))\n",
    "lab_idx, unlab_idx = idx[:LABELED_SAMPLES], idx[LABELED_SAMPLES:]\n",
    "x_lab, y_lab = x_train[lab_idx], y_train[lab_idx]\n",
    "x_unlab      = x_train[unlab_idx]\n",
    "\n",
    "def make_cnn():\n",
    "    m = models.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(28,28,1)),\n",
    "        layers.MaxPool2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPool2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "# Step 1: Initial supervised training on small labeled set\n",
    "base = make_cnn()\n",
    "base.fit(x_lab, y_lab, epochs=EPOCHS_BASE, batch_size=BATCH_SIZE, \n",
    "         validation_data=(x_test, y_test))\n",
    "base_test_acc = base.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(f\"Base test accuracy after supervised training on {LABELED_SAMPLES} samples: {base_test_acc:.4f}\")\n",
    "\n",
    "# Step 2: Generate pseudo-labels for unlabeled data\n",
    "probs = base.predict(x_unlab, batch_size=BATCH_SIZE, verbose=0)\n",
    "conf  = probs.max(axis=1)\n",
    "y_pl  = probs.argmax(axis=1)\n",
    "mask  = conf >= CONF_THRESH\n",
    "\n",
    "x_pseudo, y_pseudo = x_unlab[mask], y_pl[mask]\n",
    "print(f\"Accepted pseudo-labeled samples: {len(x_pseudo)} / {len(x_unlab)} \"\n",
    "      f\"({100*len(x_pseudo)/len(x_unlab):.1f}%)\")\n",
    "\n",
    "# Step 3: Fine-tune model on combined labeled + pseudo-labeled data\n",
    "x_mix = np.concatenate([x_lab, x_pseudo], axis=0)\n",
    "y_mix = np.concatenate([y_lab, y_pseudo], axis=0)\n",
    "\n",
    "# Continue training the same model\n",
    "finetuen = base\n",
    "finetuen.fit(x_mix, y_mix, epochs=EPOCHS_FINETUNE, batch_size=BATCH_SIZE, \n",
    "             validation_data=(x_test, y_test))\n",
    "finetune_test_acc = finetuen.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(f\"Test accuracy after fine-tuning with pseudo-labels: {finetune_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d65bea",
   "metadata": {},
   "source": [
    "Result:\n",
    "Training on just 1 000 true labels gave a baseline test accuracy of 88.6%. After adding \n",
    "~26 400 high-confidence pseudo-labels and fine-tuning, accuracy rose to 93.6%. This ~5-point jump shows that leveraging unlabeled data via pseudo-labeling can substantially improve performance when labels are scarce.\n",
    "\n",
    "Trade-off: quantity vs. quality of pseudo-labels\n",
    "We accepted 44.8% of the unlabeled pool (threshold ≥ 0.95). A high threshold ensures most pseudo-labels are correct but limits their number. Lowering the threshold would increase sample count but risk introducing noisy labels that may hurt rather than help.\n",
    "\n",
    "Model confidence matters\n",
    "The sharply improved accuracy indicates that the model’s high-confidence predictions are reliable. Early in training, some classes may be underrepresented among pseudo-labels; monitoring per-class acceptance can reveal and correct biases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
