{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3daef6",
   "metadata": {},
   "source": [
    "The book also introduced another method to AD (auto differentiation), which is forward mode accumulation. This avoid the need to store the values and gradients for the whole graph. In this module, we will visualize the whole process, which help you to better understand the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1365ae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Forward Mode Automatic Differentiation ---\n",
      "x: val=2.0, dot=1.0\n",
      "y: val=3.0, dot=0.0\n",
      "u = x * y: val=6.0, dot=3.0\n",
      "v = sin(x): val=0.9092974268256817, dot=-0.4161468365471424\n",
      "f = u + v: val=6.909297426825682, df/dx=2.5838531634528574\n",
      "df/dy=2.0\n",
      "\n",
      "--- Reverse Mode Automatic Differentiation ---\n",
      "Forward pass: x=2.0, y=3.0, u=6.0, v=0.9092974268256817, f=6.909297426825682\n",
      "Backward pass: f_bar=1.0, u_bar=1.0, v_bar=1.0\n",
      "x_bar from u=3.0, x_bar from v=-0.4161468365471424, total x_bar=2.5838531634528574\n",
      "y_bar=2.0\n",
      "\n",
      "--- Differences Between Forward and Reverse Mode ---\n",
      "1. Computation Order:\n",
      "   - Forward mode: propagate derivatives from inputs → output.\n",
      "   - Reverse mode: propagate derivatives from output → inputs.\n",
      "2. Memory Usage:\n",
      "   - Forward mode: low memory, no need to store all intermediates.\n",
      "   - Reverse mode: higher memory, must store intermediates for backward pass.\n",
      "3. Efficiency:\n",
      "   - Forward mode: efficient when few inputs, many outputs.\n",
      "   - Reverse mode: efficient when many inputs, few outputs (e.g., scalar loss).\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function: f(x, y) = x * y + sin(x)\n",
    "\n",
    "def forward_mode_autodiff(x_val, y_val):\n",
    "    print(\"--- Forward Mode Automatic Differentiation ---\")\n",
    "\n",
    "    # Derivative w.r.t x\n",
    "    x = {'val': x_val, 'dot': 1.0}  # seed derivative for x\n",
    "    y = {'val': y_val, 'dot': 0.0}  # derivative w.r.t x is 0\n",
    "\n",
    "    print(f\"x: val={x['val']}, dot={x['dot']}\")\n",
    "    print(f\"y: val={y['val']}, dot={y['dot']}\")\n",
    "\n",
    "    u = {\n",
    "        'val': x['val'] * y['val'],\n",
    "        'dot': x['dot'] * y['val'] + x['val'] * y['dot']\n",
    "    }\n",
    "    print(f\"u = x * y: val={u['val']}, dot={u['dot']}\")\n",
    "\n",
    "    v = {\n",
    "        'val': math.sin(x['val']),\n",
    "        'dot': math.cos(x['val']) * x['dot']\n",
    "    }\n",
    "    print(f\"v = sin(x): val={v['val']}, dot={v['dot']}\")\n",
    "\n",
    "    f = {\n",
    "        'val': u['val'] + v['val'],\n",
    "        'dot': u['dot'] + v['dot']\n",
    "    }\n",
    "    print(f\"f = u + v: val={f['val']}, df/dx={f['dot']}\")\n",
    "\n",
    "    # Derivative w.r.t y\n",
    "    x = {'val': x_val, 'dot': 0.0}\n",
    "    y = {'val': y_val, 'dot': 1.0}\n",
    "\n",
    "    u = {\n",
    "        'val': x['val'] * y['val'],\n",
    "        'dot': x['dot'] * y['val'] + x['val'] * y['dot']\n",
    "    }\n",
    "    v = {\n",
    "        'val': math.sin(x['val']),\n",
    "        'dot': math.cos(x['val']) * x['dot']\n",
    "    }\n",
    "    f = {\n",
    "        'val': u['val'] + v['val'],\n",
    "        'dot': u['dot'] + v['dot']\n",
    "    }\n",
    "    print(f\"df/dy={f['dot']}\")\n",
    "\n",
    "\n",
    "def reverse_mode_autodiff(x_val, y_val):\n",
    "    print(\"\\n--- Reverse Mode Automatic Differentiation ---\")\n",
    "\n",
    "    # Forward pass\n",
    "    x = x_val\n",
    "    y = y_val\n",
    "    u = x * y\n",
    "    v = math.sin(x)\n",
    "    f = u + v\n",
    "    print(f\"Forward pass: x={x}, y={y}, u={u}, v={v}, f={f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    f_bar = 1.0  # seed derivative\n",
    "    u_bar = f_bar * 1.0\n",
    "    v_bar = f_bar * 1.0\n",
    "\n",
    "    x_bar_from_u = u_bar * y\n",
    "    y_bar = u_bar * x\n",
    "    x_bar_from_v = v_bar * math.cos(x)\n",
    "\n",
    "    x_bar = x_bar_from_u + x_bar_from_v\n",
    "\n",
    "    print(f\"Backward pass: f_bar={f_bar}, u_bar={u_bar}, v_bar={v_bar}\")\n",
    "    print(f\"x_bar from u={x_bar_from_u}, x_bar from v={x_bar_from_v}, total x_bar={x_bar}\")\n",
    "    print(f\"y_bar={y_bar}\")\n",
    "\n",
    "\n",
    "def explain_differences():\n",
    "    print(\"\\n--- Differences Between Forward and Reverse Mode ---\")\n",
    "    print(\"1. Computation Order:\")\n",
    "    print(\"   - Forward mode: propagate derivatives from inputs → output.\")\n",
    "    print(\"   - Reverse mode: propagate derivatives from output → inputs.\")\n",
    "    print(\"2. Memory Usage:\")\n",
    "    print(\"   - Forward mode: low memory, no need to store all intermediates.\")\n",
    "    print(\"   - Reverse mode: higher memory, must store intermediates for backward pass.\")\n",
    "    print(\"3. Efficiency:\")\n",
    "    print(\"   - Forward mode: efficient when few inputs, many outputs.\")\n",
    "    print(\"   - Reverse mode: efficient when many inputs, few outputs (e.g., scalar loss).\")\n",
    "\n",
    "\n",
    "# Example run\n",
    "x_val = 2.0\n",
    "y_val = 3.0\n",
    "\n",
    "forward_mode_autodiff(x_val, y_val)\n",
    "reverse_mode_autodiff(x_val, y_val)\n",
    "explain_differences()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
